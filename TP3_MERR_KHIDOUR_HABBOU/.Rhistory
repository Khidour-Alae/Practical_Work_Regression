prediction <- predict(reg_log, test_data)
# compute confusion matrix
confusion_matrix <- table(as.numeric(prediction > 0.5), diabetes_data[index,]$YBin)
# compute the performance
performance[i] <- confusion_matrix[1,2] / nrow(test_data)
}
return (performance)
}
kfold_step <- function(k) # k is here the number of folds
{
# create a vector of length number of folds
performance <- vector(length = k)
# create a sequence from 1 to k
folds <- cut(seq(1,nrow(diabetes_data)), breaks = k, labels = FALSE)
# perform k fold cross validation
for(i in 1:k)
{
# split data by fold
index <- which(folds == i, arr.ind = TRUE)
test_data <- diabetes_data[index,]
train_data <- diabetes_data[-index,]
# train the logistic regression on the train data set
reg_log <- step(glm(YBin ~ ., family = binomial, data = train_data), direction = "both", trace = FALSE)
# make predictions
prediction <- predict(reg_log, test_data)
# compute confusion matrix
confusion_matrix <- table(as.numeric(prediction > 0.5), diabetes_data[index,]$YBin)
# compute the performance
performance[i] <- (confusion_matrix[1,1] + confusion_matrix[2,2]) / nrow(test_data)
}
return (performance)
}
boxplot(kfold_none(10), kfold_all(10), kfold_step(10), kfold_ridge(10, cv_ridge$lambda.min), kfold_lasso(10, cv_lasso$lambda.min),
main = "10-Fold for 5 different models",
names = c("Null", "Full", "Stepwise", "Ridge Min", "Lasso Min"),
col = c("mediumorchid3", "royalblue3", "tomato1", "goldenrod2", "seagreen"))
knitr::opts_chunk$set(echo = TRUE, comment = NA)
rm(list=ls())
graphics.off()
library(graphics)
library(ggplot2)
library(corrplot)
library(glmnet)
diabetes_data <- read.table(file = "diabetes.txt",header = TRUE)
YBin <- as.numeric(diabetes_data$Y > median(diabetes_data$Y))
diabetes_data <- diabetes_data[,-11]
diabetes_data <- cbind(diabetes_data, YBin)
head(diabetes_data)
pairs(diabetes_data, pch = 22, bg = c("tomato1","royalblue3")[unclass(factor(diabetes_data[,"YBin"]))])
corr <- cor(diabetes_data)
corrplot(corr, method = "circle")
sample <- sample(c(TRUE, FALSE), nrow(diabetes_data), replace = TRUE, prob = c(0.8, 0.2))
train_data <- diabetes_data[sample, ]
test_data <- diabetes_data[!sample, ]
reg_log = glm(formula = YBin ~ ., family = binomial, data = train_data)
summary(reg_log)
reg_log$coefficients
predict_response <- predict.glm(reg_log, newdata = test_data, type = "response")
predict_response
predict_link <- predict.glm(reg_log, newdata = test_data, type = "link")
predict_link
exp(coef(reg_log))
prediction <- as.numeric(predict.glm(reg_log, newdata = diabetes_data, type = "response") > 0.5)
table(prediction)
table(diabetes_data$YBin)
confusion_matrix <- table(diabetes_data$YBin, prediction)
confusion_matrix
accuracy <- (confusion_matrix[1,1] + confusion_matrix[2,2]) / nrow(diabetes_data)
accuracy
global_error <- (confusion_matrix[1,2] + confusion_matrix[2,1]) / nrow(diabetes_data)
global_error
recall <- confusion_matrix[2,2] / (confusion_matrix[1,2] + confusion_matrix[2,2])
recall
precision <- confusion_matrix[2,2] / (confusion_matrix[2,1] + confusion_matrix[2,2])
precision
f1_score <- (2 * precision * recall) / (precision + recall)
f1_score
confusion_matrix[2,1] / nrow(diabetes_data)
confusion_matrix[1,2] / nrow(diabetes_data)
kfold_all <- function(k) # k is here the number of folds
{
# create a vector of length number of folds
performance <- vector(length = k)
# create a sequence from 1 to k
folds <- cut(seq(1,nrow(diabetes_data)), breaks = k, labels = FALSE)
# perform k fold cross validation
for(i in 1:k)
{
# split data by fold
index <- which(folds == i, arr.ind = TRUE)
test_data <- diabetes_data[index,]
train_data <- diabetes_data[-index,]
# train the logistic regression on the train data set
reg_log <- glm(YBin ~ ., family = binomial, data = train_data)
# make predictions
prediction <- predict(reg_log, test_data)
# compute confusion matrix
confusion_matrix <- table(as.numeric(prediction > 0.5), diabetes_data[index,]$YBin)
# compute the performance
performance[i] <- (confusion_matrix[1,1] + confusion_matrix[2,2]) / nrow(test_data)
}
return (performance)
}
boxplot(kfold_all(6), kfold_all(8), kfold_all(10),
main = "K-Fold", names = c("k = 6", "k = 8", "k = 10"),
col = c("royalblue1", "tomato1", "seagreen"))
reg_all <- glm(YBin ~., data = diabetes_data, family = binomial)
reg_none <- glm(YBin ~ 1, data = diabetes_data, family = binomial)
reg_forward <- step(reg_none, list(upper = reg_all), direction = 'forward')
reg_back <- step(reg_all, direction = 'backward')
reg_both <- step(reg_all, direction = 'both')
diabetes_reg <- lm(formula(reg_both), data = diabetes_data)
summary(diabetes_reg)
sample <- sample(c(TRUE, FALSE), nrow(diabetes_data), replace = TRUE, prob = c(0.8, 0.2))
train_data <- diabetes_data[sample, ]
test_data <- diabetes_data[!sample, ]
ridge <- glmnet(x = train_data[,-11], y = train_data$YBin, alpha = 0, family = "binomial")
plot(ridge, xvar = "lambda", label = TRUE, lwd = 2)
cv_ridge <- cv.glmnet(as.matrix(train_data[,-11]), train_data$YBin,
family = "binomial", alpha = 0, type.measure = "mse")
plot(cv_ridge)
lambda_min <- cv_ridge$lambda.min
ridge_min <- glmnet(x = train_data[,-11], y = train_data$YBin,
alpha = 0, family = "binomial", lambda = lambda_min)
ridge_min$beta
prediction_min <- as.numeric(predict(ridge_min, as.matrix(diabetes_data[,-11]), type = "response") > 0.5)
table(prediction_min)
table(diabetes_data$YBin)
confusion_matrix <- table(diabetes_data$YBin, prediction_min)
confusion_matrix
accuracy <- (confusion_matrix[1,1] + confusion_matrix[2,2]) / nrow(diabetes_data)
accuracy
lambda_1se <- cv_ridge$lambda.1se
ridge_1se <- glmnet(x = train_data[,-11], y = train_data$YBin,
alpha = 0, family = "binomial", lambda = lambda_1se)
ridge_1se$beta
prediction_1se <- as.numeric(predict(ridge_1se, as.matrix(diabetes_data[,-11]), type = "response") > 0.5)
table(prediction_1se)
table(diabetes_data$YBin)
confusion_matrix <- table(diabetes_data$YBin, prediction_1se)
confusion_matrix
accuracy <- (confusion_matrix[1,1] + confusion_matrix[2,2]) / nrow(diabetes_data)
accuracy
kfold_ridge <- function(k, lambda)
{
# create a vector of length number of folds
performance <- vector(length = k)
# create a sequence from 1 to k
folds <- cut(seq(1,nrow(diabetes_data)), breaks = k, labels = FALSE)
# perform 10 fold cross validation
for(i in 1:k)
{
# split data by fold
index <- which(folds == i, arr.ind = TRUE)
test_data <- diabetes_data[index,]
train_data <- diabetes_data[-index,]
# train the logistic regression on the train data set
reg_log <- glmnet(x = train_data[,-11], y = train_data$YBin,
alpha = 0, family = "binomial", lambda = lambda)
# make predictions
prediction <- as.numeric(predict(reg_log, as.matrix(test_data[,-11]), type = "response") > 0.5)
confusion_matrix <- table(prediction, test_data$YBin)
# compute the performance
performance[i] <- (confusion_matrix[1,1] + confusion_matrix[2,2]) / nrow(test_data)
}
return (performance)
}
boxplot(kfold_ridge(10, lambda_min), kfold_ridge(10, lambda_1se), main = "K-Fold",
names = c("lambda_min", "lambda_1se"), col = c("royalblue1", "tomato1"))
lasso <- glmnet(x = train_data[,-11], y = train_data$YBin, alpha = 1, family = "binomial")
plot(lasso, xvar = "lambda", label = TRUE, lwd = 2)
cv_lasso <- cv.glmnet(as.matrix(train_data[,-11]), train_data$YBin,
family = "binomial", alpha = 1, type.measure = "mse")
plot(cv_lasso)
lambda_min <- cv_lasso$lambda.min
lasso_min <- glmnet(x = train_data[,-11], y = train_data$YBin,
alpha = 1, family = "binomial", lambda = lambda_min)
lasso_min$beta
prediction_min <- as.numeric(predict(lasso_min, as.matrix(diabetes_data[,-11]), type = "response") > 0.5)
table(prediction_min)
table(diabetes_data$YBin)
confusion_matrix <- table(diabetes_data$YBin, prediction_min)
confusion_matrix
accuracy <- (confusion_matrix[1,1] + confusion_matrix[2,2]) / nrow(diabetes_data)
accuracy
lambda_1se <- cv_lasso$lambda.1se
lasso_1se <- glmnet(x = train_data[,-11], y = train_data$YBin,
alpha = 1, family = "binomial", lambda = lambda_1se)
lasso_1se$beta
prediction_1se <- as.numeric(predict(lasso_1se, as.matrix(diabetes_data[,-11]), type = "response") > 0.5)
table(prediction_1se)
table(diabetes_data$YBin)
confusion_matrix <- table(diabetes_data$YBin, prediction_1se)
confusion_matrix
accuracy <- (confusion_matrix[1,1] + confusion_matrix[2,2]) / nrow(diabetes_data)
accuracy
kfold_lasso <- function(k, lambda)
{
# create a vector of length number of folds
performance <- vector(length = k)
# create a sequence from 1 to k
folds <- cut(seq(1,nrow(diabetes_data)), breaks = k, labels = FALSE)
# perform 10 fold cross validation
for(i in 1:k)
{
# split data by fold
index <- which(folds == i, arr.ind = TRUE)
test_data <- diabetes_data[index,]
train_data <- diabetes_data[-index,]
# train the logistic regression on the train data set
reg_log <- glmnet(x = train_data[,-11], y = train_data$YBin,
alpha = 1, family = "binomial", lambda = lambda)
# make predictions
prediction <- as.numeric(predict(reg_log, as.matrix(test_data[,-11]), type = "response") > 0.5)
confusion_matrix <- table(prediction, test_data$YBin)
# compute the performance
performance[i] <- (confusion_matrix[1,1] + confusion_matrix[2,2]) / nrow(test_data)
}
return (performance)
}
boxplot(kfold_lasso(10, lambda_min), kfold_lasso(10, lambda_1se), main = "K-Fold",
names = c("lambda_min", "lambda_1se"), col = c("royalblue1", "tomato1"))
kfold_none <- function(k) # k is here the number of folds
{
# create a vector of length number of folds
performance <- vector(length = k)
# create a sequence from 1 to k
folds <- cut(seq(1,nrow(diabetes_data)), breaks = k, labels = FALSE)
# perform k fold cross validation
for(i in 1:k)
{
# split data by fold
index <- which(folds == i, arr.ind = TRUE)
test_data <- diabetes_data[index,]
train_data <- diabetes_data[-index,]
# train the logistic regression on the train data set
reg_log <- glm(YBin ~ 1, family = binomial, data = train_data)
# make predictions
prediction <- predict(reg_log, test_data)
# compute confusion matrix
confusion_matrix <- table(as.numeric(prediction > 0.5), diabetes_data[index,]$YBin)
# compute the performance
performance[i] <- confusion_matrix[1,2] / nrow(test_data)
}
return (performance)
}
kfold_step <- function(k) # k is here the number of folds
{
# create a vector of length number of folds
performance <- vector(length = k)
# create a sequence from 1 to k
folds <- cut(seq(1,nrow(diabetes_data)), breaks = k, labels = FALSE)
# perform k fold cross validation
for(i in 1:k)
{
# split data by fold
index <- which(folds == i, arr.ind = TRUE)
test_data <- diabetes_data[index,]
train_data <- diabetes_data[-index,]
# train the logistic regression on the train data set
reg_log <- step(glm(YBin ~ ., family = binomial, data = train_data), direction = "both", trace = FALSE)
# make predictions
prediction <- predict(reg_log, test_data)
# compute confusion matrix
confusion_matrix <- table(as.numeric(prediction > 0.5), diabetes_data[index,]$YBin)
# compute the performance
performance[i] <- (confusion_matrix[1,1] + confusion_matrix[2,2]) / nrow(test_data)
}
return (performance)
}
boxplot(kfold_none(10), kfold_all(10), kfold_step(10), kfold_ridge(10, cv_ridge$lambda.min), kfold_lasso(10, cv_lasso$lambda.min),
main = "10-Fold for 5 different models",
names = c("Null", "Full", "Stepwise", "Ridge Min", "Lasso Min"),
col = c("mediumorchid3", "royalblue3", "tomato1", "goldenrod2", "seagreen"))
knitr::opts_chunk$set(echo = TRUE, comment = NA)
rm(list=ls())
graphics.off()
library(graphics)
library(ggplot2)
library(corrplot)
library(glmnet)
diabetes_data <- read.table(file = "diabetes.txt",header = TRUE)
YBin <- as.numeric(diabetes_data$Y > median(diabetes_data$Y))
diabetes_data <- diabetes_data[,-11]
diabetes_data <- cbind(diabetes_data, YBin)
head(diabetes_data)
pairs(diabetes_data, pch = 22, bg = c("tomato1","royalblue3")[unclass(factor(diabetes_data[,"YBin"]))])
corr <- cor(diabetes_data)
corrplot(corr, method = "circle")
sample <- sample(c(TRUE, FALSE), nrow(diabetes_data), replace = TRUE, prob = c(0.8, 0.2))
train_data <- diabetes_data[sample, ]
test_data <- diabetes_data[!sample, ]
reg_log = glm(formula = YBin ~ ., family = binomial, data = train_data)
summary(reg_log)
reg_log$coefficients
predict_response <- predict.glm(reg_log, newdata = test_data, type = "response")
predict_response
predict_link <- predict.glm(reg_log, newdata = test_data, type = "link")
predict_link
exp(coef(reg_log))
prediction <- as.numeric(predict.glm(reg_log, newdata = diabetes_data, type = "response") > 0.5)
table(prediction)
table(diabetes_data$YBin)
confusion_matrix <- table(diabetes_data$YBin, prediction)
confusion_matrix
accuracy <- (confusion_matrix[1,1] + confusion_matrix[2,2]) / nrow(diabetes_data)
accuracy
global_error <- (confusion_matrix[1,2] + confusion_matrix[2,1]) / nrow(diabetes_data)
global_error
recall <- confusion_matrix[2,2] / (confusion_matrix[1,2] + confusion_matrix[2,2])
recall
precision <- confusion_matrix[2,2] / (confusion_matrix[2,1] + confusion_matrix[2,2])
precision
f1_score <- (2 * precision * recall) / (precision + recall)
f1_score
confusion_matrix[2,1] / nrow(diabetes_data)
confusion_matrix[1,2] / nrow(diabetes_data)
kfold_all <- function(k) # k is here the number of folds
{
# create a vector of length number of folds
performance <- vector(length = k)
# create a sequence from 1 to k
folds <- cut(seq(1,nrow(diabetes_data)), breaks = k, labels = FALSE)
# perform k fold cross validation
for(i in 1:k)
{
# split data by fold
index <- which(folds == i, arr.ind = TRUE)
test_data <- diabetes_data[index,]
train_data <- diabetes_data[-index,]
# train the logistic regression on the train data set
reg_log <- glm(YBin ~ ., family = binomial, data = train_data)
# make predictions
prediction <- predict(reg_log, test_data)
# compute confusion matrix
confusion_matrix <- table(as.numeric(prediction > 0.5), diabetes_data[index,]$YBin)
# compute the performance
performance[i] <- (confusion_matrix[1,1] + confusion_matrix[2,2]) / nrow(test_data)
}
return (performance)
}
boxplot(kfold_all(6), kfold_all(8), kfold_all(10),
main = "K-Fold", names = c("k = 6", "k = 8", "k = 10"),
col = c("royalblue1", "tomato1", "seagreen"))
reg_all <- glm(YBin ~., data = diabetes_data, family = binomial)
reg_none <- glm(YBin ~ 1, data = diabetes_data, family = binomial)
reg_forward <- step(reg_none, list(upper = reg_all), direction = 'forward')
reg_back <- step(reg_all, direction = 'backward')
reg_both <- step(reg_all, direction = 'both')
diabetes_reg <- lm(formula(reg_both), data = diabetes_data)
summary(diabetes_reg)
sample <- sample(c(TRUE, FALSE), nrow(diabetes_data), replace = TRUE, prob = c(0.8, 0.2))
train_data <- diabetes_data[sample, ]
test_data <- diabetes_data[!sample, ]
ridge <- glmnet(x = train_data[,-11], y = train_data$YBin, alpha = 0, family = "binomial")
plot(ridge, xvar = "lambda", label = TRUE, lwd = 2)
cv_ridge <- cv.glmnet(as.matrix(train_data[,-11]), train_data$YBin,
family = "binomial", alpha = 0, type.measure = "mse")
plot(cv_ridge)
lambda_min <- cv_ridge$lambda.min
ridge_min <- glmnet(x = train_data[,-11], y = train_data$YBin,
alpha = 0, family = "binomial", lambda = lambda_min)
ridge_min$beta
prediction_min <- as.numeric(predict(ridge_min, as.matrix(diabetes_data[,-11]), type = "response") > 0.5)
table(prediction_min)
table(diabetes_data$YBin)
confusion_matrix <- table(diabetes_data$YBin, prediction_min)
confusion_matrix
accuracy <- (confusion_matrix[1,1] + confusion_matrix[2,2]) / nrow(diabetes_data)
accuracy
lambda_1se <- cv_ridge$lambda.1se
ridge_1se <- glmnet(x = train_data[,-11], y = train_data$YBin,
alpha = 0, family = "binomial", lambda = lambda_1se)
ridge_1se$beta
prediction_1se <- as.numeric(predict(ridge_1se, as.matrix(diabetes_data[,-11]), type = "response") > 0.5)
table(prediction_1se)
table(diabetes_data$YBin)
confusion_matrix <- table(diabetes_data$YBin, prediction_1se)
confusion_matrix
accuracy <- (confusion_matrix[1,1] + confusion_matrix[2,2]) / nrow(diabetes_data)
accuracy
kfold_ridge <- function(k, lambda)
{
# create a vector of length number of folds
performance <- vector(length = k)
# create a sequence from 1 to k
folds <- cut(seq(1,nrow(diabetes_data)), breaks = k, labels = FALSE)
# perform 10 fold cross validation
for(i in 1:k)
{
# split data by fold
index <- which(folds == i, arr.ind = TRUE)
test_data <- diabetes_data[index,]
train_data <- diabetes_data[-index,]
# train the logistic regression on the train data set
reg_log <- glmnet(x = train_data[,-11], y = train_data$YBin,
alpha = 0, family = "binomial", lambda = lambda)
# make predictions
prediction <- as.numeric(predict(reg_log, as.matrix(test_data[,-11]), type = "response") > 0.5)
confusion_matrix <- table(prediction, test_data$YBin)
# compute the performance
performance[i] <- (confusion_matrix[1,1] + confusion_matrix[2,2]) / nrow(test_data)
}
return (performance)
}
boxplot(kfold_ridge(10, lambda_min), kfold_ridge(10, lambda_1se), main = "K-Fold",
names = c("lambda_min", "lambda_1se"), col = c("royalblue1", "tomato1"))
lasso <- glmnet(x = train_data[,-11], y = train_data$YBin, alpha = 1, family = "binomial")
plot(lasso, xvar = "lambda", label = TRUE, lwd = 2)
cv_lasso <- cv.glmnet(as.matrix(train_data[,-11]), train_data$YBin,
family = "binomial", alpha = 1, type.measure = "mse")
plot(cv_lasso)
lambda_min <- cv_lasso$lambda.min
lasso_min <- glmnet(x = train_data[,-11], y = train_data$YBin,
alpha = 1, family = "binomial", lambda = lambda_min)
lasso_min$beta
prediction_min <- as.numeric(predict(lasso_min, as.matrix(diabetes_data[,-11]), type = "response") > 0.5)
table(prediction_min)
table(diabetes_data$YBin)
confusion_matrix <- table(diabetes_data$YBin, prediction_min)
confusion_matrix
accuracy <- (confusion_matrix[1,1] + confusion_matrix[2,2]) / nrow(diabetes_data)
accuracy
lambda_1se <- cv_lasso$lambda.1se
lasso_1se <- glmnet(x = train_data[,-11], y = train_data$YBin,
alpha = 1, family = "binomial", lambda = lambda_1se)
lasso_1se$beta
prediction_1se <- as.numeric(predict(lasso_1se, as.matrix(diabetes_data[,-11]), type = "response") > 0.5)
table(prediction_1se)
table(diabetes_data$YBin)
confusion_matrix <- table(diabetes_data$YBin, prediction_1se)
confusion_matrix
accuracy <- (confusion_matrix[1,1] + confusion_matrix[2,2]) / nrow(diabetes_data)
accuracy
kfold_lasso <- function(k, lambda)
{
# create a vector of length number of folds
performance <- vector(length = k)
# create a sequence from 1 to k
folds <- cut(seq(1,nrow(diabetes_data)), breaks = k, labels = FALSE)
# perform 10 fold cross validation
for(i in 1:k)
{
# split data by fold
index <- which(folds == i, arr.ind = TRUE)
test_data <- diabetes_data[index,]
train_data <- diabetes_data[-index,]
# train the logistic regression on the train data set
reg_log <- glmnet(x = train_data[,-11], y = train_data$YBin,
alpha = 1, family = "binomial", lambda = lambda)
# make predictions
prediction <- as.numeric(predict(reg_log, as.matrix(test_data[,-11]), type = "response") > 0.5)
confusion_matrix <- table(prediction, test_data$YBin)
# compute the performance
performance[i] <- (confusion_matrix[1,1] + confusion_matrix[2,2]) / nrow(test_data)
}
return (performance)
}
boxplot(kfold_lasso(10, lambda_min), kfold_lasso(10, lambda_1se), main = "K-Fold",
names = c("lambda_min", "lambda_1se"), col = c("royalblue1", "tomato1"))
kfold_none <- function(k) # k is here the number of folds
{
# create a vector of length number of folds
performance <- vector(length = k)
# create a sequence from 1 to k
folds <- cut(seq(1,nrow(diabetes_data)), breaks = k, labels = FALSE)
# perform k fold cross validation
for(i in 1:k)
{
# split data by fold
index <- which(folds == i, arr.ind = TRUE)
test_data <- diabetes_data[index,]
train_data <- diabetes_data[-index,]
# train the logistic regression on the train data set
reg_log <- glm(YBin ~ 1, family = binomial, data = train_data)
# make predictions
prediction <- predict(reg_log, test_data)
# compute confusion matrix
confusion_matrix <- table(as.numeric(prediction > 0.5), diabetes_data[index,]$YBin)
# compute the performance
performance[i] <- confusion_matrix[1,2] / nrow(test_data)
}
return (performance)
}
kfold_step <- function(k) # k is here the number of folds
{
# create a vector of length number of folds
performance <- vector(length = k)
# create a sequence from 1 to k
folds <- cut(seq(1,nrow(diabetes_data)), breaks = k, labels = FALSE)
# perform k fold cross validation
for(i in 1:k)
{
# split data by fold
index <- which(folds == i, arr.ind = TRUE)
test_data <- diabetes_data[index,]
train_data <- diabetes_data[-index,]
# train the logistic regression on the train data set
reg_log <- step(glm(YBin ~ ., family = binomial, data = train_data), direction = "both", trace = FALSE)
# make predictions
prediction <- predict(reg_log, test_data)
# compute confusion matrix
confusion_matrix <- table(as.numeric(prediction > 0.5), diabetes_data[index,]$YBin)
# compute the performance
performance[i] <- (confusion_matrix[1,1] + confusion_matrix[2,2]) / nrow(test_data)
}
return (performance)
}
boxplot(kfold_none(10), kfold_all(10), kfold_step(10), kfold_ridge(10, cv_ridge$lambda.min), kfold_lasso(10, cv_lasso$lambda.min),
main = "10-Fold for 5 different models",
names = c("Null", "Full", "Stepwise", "Ridge Min", "Lasso Min"),
col = c("mediumorchid3", "royalblue3", "tomato1", "goldenrod2", "seagreen"))
